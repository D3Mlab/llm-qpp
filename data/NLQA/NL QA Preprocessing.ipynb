{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf913903-97a2-4761-90b5-3ff72a311bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dad87c10-f7e7-46a8-b70b-ba3325c75726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    _id              title                                               text  \\\n",
      "0  doc0  Minority interest  In accounting, minority interest (or non-contr...   \n",
      "1  doc1  Minority interest  It is, however, possible (such as through spec...   \n",
      "2  doc2  Minority interest  The reporting of 'minority interest' is a cons...   \n",
      "3  doc3  Minority interest  Some investors have expressed concern that the...   \n",
      "4  doc4  Minority interest  Minority interest is an integral part of the e...   \n",
      "\n",
      "  metadata  \n",
      "0       {}  \n",
      "1       {}  \n",
      "2       {}  \n",
      "3       {}  \n",
      "4       {}  \n"
     ]
    }
   ],
   "source": [
    "#inspect nl qa formats\n",
    "file_path = 'C:/Users/anton/source/data/LLM-QPP/nq/corpus.jsonl'\n",
    "df = pd.read_json(file_path, lines=True, nrows=5)  # Reads only 5 lines\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fae25c7-53f0-4b28-b214-687ff2982e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     _id                                               text metadata\n",
      "0  test0  what is non controlling interest on balance sheet       {}\n",
      "1  test1     how many episodes are in chicago fire season 4       {}\n",
      "2  test2    who sings love will keep us alive by the eagles       {}\n",
      "3  test3          who is the leader of the ontario pc party       {}\n",
      "4  test4    nitty gritty dirt band fishin in the dark album       {}\n"
     ]
    }
   ],
   "source": [
    "file_path = 'C:/Users/anton/source/data/LLM-QPP/nq/queries.jsonl'\n",
    "df = pd.read_json(file_path, lines=True, nrows=5)  # Reads only 5 lines\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f5a98e-06b9-4525-8039-b52864dd135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qrels:\n",
    "#query-id\tcorpus-id\tscore\n",
    "#test0\tdoc0\t1\n",
    "#test0\tdoc1\t1\n",
    "#test1\tdoc6\t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a81e9481-1c9e-4c11-8cbb-b6f009008691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a smaller subset of dataset\n",
    "#select a small subset (e.g. 100 queries) from the development set\n",
    "#generate a new qrels file based on the selected queries\n",
    "#build the document corpus by:\n",
    "# - only considering queries for which there is a qrels entry (about half are missing)\n",
    "# - adding all documents marked relevant to the subset of queries in qrels\n",
    "# - sampling X (e.g 10K) random documents not already in qrels\n",
    "#- save all into same original .tsv format with same qIDs and dIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1559f800-bbc5-42cb-be82-71dfa14cccb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def generate_nlqa_subset(queries_file, qrels_file, corpus_file, output_queries_file, output_qrels_file, output_corpus_file, num_queries=100, num_random_docs=10000):\n",
    "    \"\"\"\n",
    "    Generate a smaller subset of the NLQA dataset including queries (TSV), qrels (TSV), and documents (JSONL).\n",
    "    The output corpus JSONL will align with MS MARCO format: 'docID' and 'text'.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    output_dir = os.path.dirname(output_queries_file)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Load the queries JSONL\n",
    "    queries_list = []\n",
    "    with open(queries_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                queries_list.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping malformed line: {line.strip()}\")\n",
    "    queries = pd.DataFrame(queries_list)\n",
    "    queries.rename(columns={\"_id\": \"qID\"}, inplace=True)\n",
    "\n",
    "    # Load qrels TSV\n",
    "    qrels = pd.read_csv(qrels_file, sep=\"\\t\", header=0, names=[\"qID\", \"docID\", \"score\"], dtype={\"qID\": str, \"docID\": str, \"score\": int})\n",
    "    \n",
    "    # Filter queries to include only those with relevant documents in qrels\n",
    "    valid_query_ids = qrels[\"qID\"].unique()\n",
    "    filtered_queries = queries[queries[\"qID\"].isin(valid_query_ids)]\n",
    "    \n",
    "    # Sample a subset of queries\n",
    "    subset_queries = filtered_queries.sample(n=num_queries, random_state=42)\n",
    "    subset_query_ids = set(subset_queries[\"qID\"])\n",
    "\n",
    "    # Filter qrels for selected queries\n",
    "    subset_qrels = qrels[qrels[\"qID\"].isin(subset_query_ids)]\n",
    "\n",
    "    # Get relevant document IDs\n",
    "    relevant_doc_ids = set(subset_qrels[\"docID\"])\n",
    "\n",
    "    # Load and process corpus JSONL\n",
    "    relevant_docs = []\n",
    "    non_relevant_docs = []\n",
    "    with open(corpus_file, 'r', encoding=\"utf-8\") as corpus:\n",
    "        for line in corpus:\n",
    "            doc = json.loads(line)\n",
    "            docID = doc[\"_id\"]\n",
    "            if docID in relevant_doc_ids:\n",
    "                # Align with MS MARCO format: Rename '_id' to 'docID' and prepend title to text\n",
    "                processed_doc = {\n",
    "                    \"docID\": doc[\"_id\"],\n",
    "                    \"text\": f\"{doc['title']}. {doc['text']}\"\n",
    "                }\n",
    "                relevant_docs.append(processed_doc)\n",
    "            else:\n",
    "                non_relevant_docs.append(doc)\n",
    "\n",
    "    # Sample additional non-relevant documents\n",
    "    sampled_non_relevant_docs = pd.DataFrame(non_relevant_docs).sample(n=num_random_docs, random_state=42).to_dict(orient=\"records\")\n",
    "\n",
    "    final_corpus = relevant_docs + [\n",
    "        {\"docID\": doc[\"_id\"], \"text\": f\"{doc['title']}. {doc['text']}\"}\n",
    "        for doc in sampled_non_relevant_docs\n",
    "    ]\n",
    "\n",
    "    # Save output files\n",
    "    # Save queries as TSV\n",
    "    subset_queries[[\"qID\", \"text\"]].to_csv(output_queries_file, sep=\"\\t\", index=False, header=False)\n",
    "    # Save qrels as TSV\n",
    "    subset_qrels.insert(1, \"zero\", 0)\n",
    "    subset_qrels.to_csv(output_qrels_file, sep=\"\\t\", index=False, header=False)\n",
    "    # Save documents as JSONL\n",
    "    with open(output_corpus_file, 'w') as f:\n",
    "        for doc in final_corpus:\n",
    "            f.write(json.dumps(doc) + \"\\n\")\n",
    "\n",
    "    print(\"NLQA subset generation complete.\")\n",
    "\n",
    "# Example usage:\n",
    "# generate_nlqa_subset(\n",
    "#     queries_file='path/to/queries.jsonl',\n",
    "#     qrels_file='path/to/qrels.tsv',\n",
    "#     corpus_file='path/to/corpus.jsonl',\n",
    "#     output_queries_file='output/queries_subset.tsv',\n",
    "#     output_qrels_file='output/qrels_subset.tsv',\n",
    "#     output_corpus_file='output/collection_subset.jsonl',\n",
    "#     num_queries=100,\n",
    "#     num_random_docs=10000\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "667f7d61-878e-40c3-b1aa-640519d5a27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLQA subset generation complete.\n"
     ]
    }
   ],
   "source": [
    "num_queries = 100 \n",
    "num_random_docs = 100000\n",
    "\n",
    "original_dir = 'C:/Users/anton/source/data/LLM-QPP/nq'\n",
    "\n",
    "# Example usage\n",
    "generate_nlqa_subset(\n",
    "    queries_file=f\"{original_dir}/queries.jsonl\",\n",
    "    qrels_file=f\"{original_dir}/qrels/test.tsv\",\n",
    "    corpus_file=f\"{original_dir}/corpus.jsonl\",\n",
    "    output_queries_file=f\"subset_q{num_queries}_d{num_random_docs}/queries.tsv\",\n",
    "    output_qrels_file=f\"subset_q{num_queries}_d{num_random_docs}/qrels.qrels\",\n",
    "    output_corpus_file=f\"subset_q{num_queries}_d{num_random_docs}/collection.jsonl\",\n",
    "    num_queries=num_queries,\n",
    "    num_random_docs=num_random_docs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50f366a5-99d7-4e35-939a-62c75323625c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     docID                                               text\n",
      "0   doc449  Wake Island. With the annexation of Hawaii in ...\n",
      "1   doc450  Wake Island. On January 17, 1899, under orders...\n",
      "2   doc908  Bull. Other than the few bulls needed for bree...\n",
      "3   doc916  Bull. Many cattle ranches and stations run bul...\n",
      "4  doc1420  Jesse Bennett. Dr. Jesse Bennett (July 10, 176...\n"
     ]
    }
   ],
   "source": [
    "# Read the JSONL file into a DataFrame\n",
    "output_documents_file = \"C:/Users/anton/source/repos/llm-qpp/data/NLQA/subset_q100_d10000/collection.jsonl\"\n",
    "df_documents = pd.read_json(output_documents_file, lines=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df_documents.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d2b8765-39e4-4d38-9727-4aed6052c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def count_docids_in_corpus(corpus_file):\n",
    "    \"\"\"\n",
    "    Count the total number of document IDs and unique document IDs in the corpus.jsonl file.\n",
    "\n",
    "    Arguments:\n",
    "    - corpus_file: Path to the corpus.jsonl file.\n",
    "    \"\"\"\n",
    "    doc_ids = []\n",
    "\n",
    "    # Load document IDs\n",
    "    with open(corpus_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            doc_ids.append(doc[\"_id\"])\n",
    "\n",
    "    # Calculate counts\n",
    "    total_doc_ids = len(doc_ids)\n",
    "    unique_doc_ids = len(set(doc_ids))\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Total number of docIDs: {total_doc_ids}\")\n",
    "    print(f\"Number of unique docIDs: {unique_doc_ids}\")\n",
    "    print(f\"Number of duplicate docIDs: {total_doc_ids - unique_doc_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7211d51-c3e8-4253-b1b2-beefad9d6acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of docIDs: 2681468\n",
      "Number of unique docIDs: 2681468\n",
      "Number of duplicate docIDs: 0\n"
     ]
    }
   ],
   "source": [
    "corpus_file=f\"{original_dir}/corpus.jsonl\"\n",
    "count_docids_in_corpus(corpus_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
