{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf913903-97a2-4761-90b5-3ff72a311bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e9481-1c9e-4c11-8cbb-b6f009008691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a tiny subset of MS-MARCO\n",
    "#select a small subset (e.g. 100 queries) from the development set\n",
    "#generate a new qrels file based on the selected queries\n",
    "#build the document corpus by:\n",
    "# - only considering queries for which there is a qrels entry (about half are missing)\n",
    "# - adding all documents marked relevant to the subset of queries in qrels\n",
    "# - sampling X (e.g 10K) random documents not already in qrels\n",
    "#- save all into same original .tsv format with same qIDs and dIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b23fc95-bdeb-4f8d-91e1-09311bf668db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_subset(queries_file, qrels_file, documents_file, output_queries_file, output_qrels_file, output_documents_file, subset_size=100, num_random_docs=10000):\n",
    "    # Generate a smaller subset of the dataset including queries, qrels, and documents\n",
    "    \n",
    "    # Arguments:\n",
    "    # queries_file: str - Path to the queries TSV file.\n",
    "    # qrels_file: str - Path to the qrels TSV file.\n",
    "    # documents_file: str - Path to the documents TSV file.\n",
    "    # output_queries_file: str - Path to save the subset queries TSV file.\n",
    "    # output_qrels_file: str - Path to save the subset qrels TSV file.\n",
    "    # output_documents_file: str - Path to save the subset documents TSV file.\n",
    "    # subset_size: int - Number of queries to include in the subset (default is 100).\n",
    "    # num_random_docs: int - Number of random non-relevant documents to include (default is 10000).\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir = os.path.dirname(output_queries_file)\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    # Load the queries\n",
    "    df_queries = pd.read_csv(queries_file, sep=\"\\t\", header=None, names=[\"qID\", \"text\"], dtype={\"qID\": str, \"text\": str})\n",
    "\n",
    "    # Load the qrels\n",
    "    df_qrels = pd.read_csv(qrels_file, sep=\"\\t\", header=None, names=[\"qID\", \"zero\", \"docID\", \"binary_relevance\"], dtype={\"qID\": str, \"docID\": str, \"binary_relevance\": int})\n",
    "\n",
    "    # Filter queries to include only those that have relevant documents in qrels\n",
    "    valid_query_ids = df_qrels[\"qID\"].unique()\n",
    "    df_queries = df_queries[df_queries[\"qID\"].isin(valid_query_ids)]\n",
    "\n",
    "    # Select a random subset of queries that have relevant documents\n",
    "    subset_queries = df_queries.sample(n=subset_size, random_state=42)\n",
    "    subset_query_ids = set(subset_queries[\"qID\"])\n",
    "\n",
    "    # Filter qrels to include only those for the selected queries\n",
    "    subset_qrels = df_qrels[df_qrels[\"qID\"].isin(subset_query_ids)]\n",
    "\n",
    "    # Get the set of relevant document IDs from the filtered qrels\n",
    "    relevant_doc_ids = set(subset_qrels[\"docID\"])\n",
    "\n",
    "    # Load document IDs only to filter relevant and non-relevant documents\n",
    "    chunk_iterator = pd.read_csv(documents_file, sep=\"\\t\", header=None, names=[\"docID\", \"text\"], dtype={\"docID\": str, \"text\": str}, usecols=[0], chunksize=1000000)\n",
    "\n",
    "    relevant_docs = []\n",
    "    non_relevant_docs = []\n",
    "\n",
    "    for chunk in chunk_iterator:\n",
    "        relevant_chunk = chunk[chunk[\"docID\"].isin(relevant_doc_ids)]\n",
    "        non_relevant_chunk = chunk[~chunk[\"docID\"].isin(relevant_doc_ids)]\n",
    "        relevant_docs.append(relevant_chunk)\n",
    "        non_relevant_docs.append(non_relevant_chunk)\n",
    "\n",
    "    # Combine all relevant documents\n",
    "    df_relevant_documents = pd.concat(relevant_docs)\n",
    "\n",
    "    # Sample additional random documents that are not already in the relevant set\n",
    "    non_relevant_docs_combined = pd.concat(non_relevant_docs)\n",
    "    random_docs = non_relevant_docs_combined.sample(n=num_random_docs, random_state=42)\n",
    "\n",
    "    # Load the text for the filtered documents\n",
    "    doc_ids_to_load = set(df_relevant_documents[\"docID\"]).union(set(random_docs[\"docID\"]))\n",
    "    chunk_iterator = pd.read_csv(documents_file, sep=\"\\t\", header=None, names=[\"docID\", \"text\"], dtype={\"docID\": str, \"text\": str}, chunksize=1000000)\n",
    "\n",
    "    final_documents = []\n",
    "    for chunk in chunk_iterator:\n",
    "        filtered_chunk = chunk[chunk[\"docID\"].isin(doc_ids_to_load)]\n",
    "        final_documents.append(filtered_chunk)\n",
    "\n",
    "    final_documents = pd.concat(final_documents).drop_duplicates(subset=[\"docID\"])\n",
    "\n",
    "    # Save the subset queries, qrels, and documents\n",
    "    subset_queries.to_csv(output_queries_file, sep=\"\\t\", index=False, header=False)\n",
    "    subset_qrels.to_csv(output_qrels_file, sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "\n",
    "    # Save documents to JSONL\n",
    "    with open(output_documents_file, 'w') as f:\n",
    "        for _, row in final_documents.iterrows():\n",
    "            json_record = {\"docID\": row[\"docID\"], \"text\": row[\"text\"]}\n",
    "            f.write(json.dumps(json_record) + \"\\n\")\n",
    "\n",
    "    print(\"Subset generation complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "667f7d61-878e-40c3-b1aa-640519d5a27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset generation complete.\n"
     ]
    }
   ],
   "source": [
    "subset_size = 10\n",
    "num_random_docs = 100\n",
    "\n",
    "original_dir = 'C:/Users/anton/source/data/LLM-QPP/MSMARCO/original'\n",
    "\n",
    "# Example usage\n",
    "generate_subset(\n",
    "    queries_file=f\"{original_dir}/queries/queries.dev.tsv\",\n",
    "    qrels_file=f\"{original_dir}/qrels.dev.tsv\",\n",
    "    documents_file=f\"{original_dir}/collection/collection.tsv\",\n",
    "    output_queries_file=f\"subset_q{subset_size}_d{num_random_docs}/queries.tsv\",\n",
    "    output_qrels_file=f\"subset_q{subset_size}_d{num_random_docs}/qrels.qrels\",\n",
    "    output_documents_file=f\"subset_q{subset_size}_d{num_random_docs}/collection.jsonl\",\n",
    "    subset_size=10,\n",
    "    num_random_docs=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50f366a5-99d7-4e35-939a-62c75323625c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    docID                                               text\n",
      "0  108035  Ever wonder how much people playing music on t...\n",
      "1  159521  Hospitality industry. The hospitality industry...\n",
      "2  175537  A tremor is a repetitive movement of a part of...\n",
      "3  210222  Hardware is the physical parts of the computer...\n",
      "4  319963  Chronic Neutropenia Individuals with the diagn...\n"
     ]
    }
   ],
   "source": [
    "# Read the JSONL file into a DataFrame\n",
    "output_documents_file = \"subsetTEST_q10_d100/collection.jsonl\"\n",
    "df_documents = pd.read_json(output_documents_file, lines=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df_documents.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
